{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "*Phase 1: Alien Pet Health, Data Preparation*\n",
        "\n",
        "# Project Context\n",
        "\n",
        "\n",
        "# Report: Alien Pet Health Data Preparation\n",
        "\n",
        "This report documents the comprehensive data preparation process for the Alien Pet Health dataset. The analysis includes data loading, missing value handling, categorical attribute normalization, feature selection, distribution analysis, class balance evaluation, and final data export.\n",
        "\n",
        "## 1. Data\n",
        "\n",
        "The dataset for Phase 1 can be found here:\n",
        "\n",
        "\n",
        "In your notebook, you can access and read the data directly from this GitHub repository.\n",
        "\n",
        "\n",
        "## 2. Tasks\n",
        "\n",
        "1. **Load the dataset**\n",
        "\n",
        "\t- Read the CSV file from the provided GitHub URL.\n",
        "\t- Show the shape of the data, as well as the first five rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2.1: Load the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"data/alien_pet_health.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Missing values**\n",
        "\n",
        "\t- Examine the dataset to identify and assess missing values in various attributes. Missing values may be represented by symbols such as ‘?’, empty strings, or other placeholders.\n",
        "\t- List the attribute or attributes with missing values.\n",
        "\t- Describe the methodology used for this investigation, and provide the corresponding code, if applicable.\n",
        "\t- Convert missing tokens (e.g., empty strings, `n/a`, `?`) to `NaN`.\n",
        "\t- Coerce numeric-like columns to numeric (errors→`NaN`).\n",
        "\n",
        "\tFollowing this step, each attribute will be populated with either specific values or `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Overall data structure:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"Missing values:\")\n",
        "missing_counts = df.isnull().sum()\n",
        "print(missing_counts[missing_counts > 0])\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "for col in df.columns:\n",
        "    unique_vals = df[col].unique()\n",
        "    print(f\"\\n{col}: {len(unique_vals)} unique values\")\n",
        "    if len(unique_vals) <= 20:\n",
        "        print(f\"Values: {unique_vals}\")\n",
        "    else:\n",
        "        print(f\"Sample values: {unique_vals[:10]}\")\n",
        "\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "missing_tokens = ['', ' ', 'n/a', 'N/A', 'na', 'NA', '?', 'null', 'NULL']\n",
        "for token in missing_tokens:\n",
        "    df_clean = df_clean.replace(token, np.nan)\n",
        "\n",
        "numeric_cols = ['thermoreg_reading', 'enzyme_activity_index', 'dual_lobe_signal', \n",
        "                'stress_variability', 'activity_score', 'fasting_flag', \n",
        "                'health_outcome', 'ingest_marker', 'diagnostic_noise', \n",
        "                'thermoreg_reading_fahrenheit']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "final_missing = df_clean.isnull().sum()\n",
        "print(final_missing[final_missing > 0])\n",
        "\n",
        "df = df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2.2\n",
        "\n",
        "Used `df.info()` to examine data structure\n",
        "Analyzed unique values\n",
        "Identified missing value tokens: `'', ' ', 'n/a', 'N/A', 'na', 'NA', '?', 'null', 'NULL'`\n",
        "Applied `pd.to_numeric()` with `errors='coerce'`\n",
        "\n",
        "All columns except `health_outcome` contain missing values\n",
        "`thermoreg_reading_fahrenheit` (949 missing), `record_id` (283 missing)\n",
        "Some missing values were represented as text tokens ('?', 'N/a') rather than NaN\n",
        "Several numeric columns contained non-numeric values that needed coercion\n",
        "\n",
        "All missing values are now standardized as NaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Categorical attributes**\n",
        "\n",
        "\t- Analyze the dataset to detect potential issues with categorical attributes. For example, you may encounter instances where the same category is inconsistently represented using both lowercase and uppercase letters, or where extraneous spaces are included.\n",
        "\t- Describe the methodology used for this investigation, and provide the corresponding code, if applicable.\n",
        "\t- Normalize the values of categorical attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = ['record_id', 'habitat_zone', 'station_code', 'calibration_tag']\n",
        "\n",
        "print(\"Columns analysis:\")\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        unique_vals = df[col].dropna().unique()\n",
        "        print(f\"{col}: {unique_vals}\")\n",
        "\n",
        "print(\"\\nNormalizing categorical values:\")\n",
        "df['habitat_zone'] = df['habitat_zone'].str.lower()\n",
        "df['station_code'] = df['station_code'].str.upper()\n",
        "df['calibration_tag'] = df['calibration_tag'].str.upper()\n",
        "\n",
        "print(\"After normalization:\")\n",
        "for col in ['habitat_zone', 'station_code', 'calibration_tag']:\n",
        "    if col in df.columns:\n",
        "        unique_vals = df[col].dropna().unique()\n",
        "        print(f\"{col}: {unique_vals}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2.3\n",
        "\n",
        "- Identified categorical columns: `record_id`, `habitat_zone`, `station_code`, `calibration_tag`\n",
        "- Examined unique values to detect inconsistencies in formatting\n",
        "- Applied string normalization functions: `.str.lower()` and `.str.upper()`\n",
        "\n",
        "#### Issues\n",
        "- Mixed case representation (c1, C1, c2, C2, etc.)\n",
        "- Inconsistent capitalization (z-eat vs Z-EAT)\n",
        "- Mixed case values (A, a, B, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Remove non-informative attributes**\n",
        "\n",
        "\t- Eliminate the following types of attributes from the dataset, if applicable:\n",
        "\t  - Unique identifiers (IDs)\n",
        "\t  - Constant and quasi-constant features\n",
        "\t  - High-cardinality quasi-identifiers\n",
        "\t  - Scaled linear duplicates\n",
        "\t- Provide the list of the specific attributes being removed.\n",
        "\t- For each attribute listed, offer a brief justification for its exclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"record_id: {df['record_id'].nunique()} unique values out of {len(df)} rows\")\n",
        "print(f\"station_code: {df['station_code'].nunique()} unique values\")\n",
        "print(f\"ingest_marker: {df['ingest_marker'].value_counts()}\")\n",
        "\n",
        "correlation_matrix = df[['thermoreg_reading', 'thermoreg_reading_fahrenheit']].corr()\n",
        "print(f\"Correlation between thermoreg_reading and fahrenheit: {correlation_matrix.iloc[0,1]:.4f}\")\n",
        "\n",
        "to_remove = ['record_id', 'station_code', 'ingest_marker', 'thermoreg_reading_fahrenheit']\n",
        "\n",
        "print(\"record_id: Unique identifier\")\n",
        "print(\"station_code: High-cardinality quasi-identifier\") \n",
        "print(\"ingest_marker: Quasi-constant (mostly 1.0)\")\n",
        "print(\"thermoreg_reading_fahrenheit: Scaled duplicate of thermoreg_reading\")\n",
        "\n",
        "df_filtered = df.drop(columns=to_remove)\n",
        "print(f\"\\nDataset shape after removal: {df_filtered.shape}\")\n",
        "\n",
        "df = df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2.4\n",
        "\n",
        "- Analyzed cardinality and variability of each attribute\n",
        "- Computed correlation between potential duplicate features\n",
        "\n",
        "1. record_id - (4,714 unique values out of 5,000 rows) - Unique identifier\n",
        "2. station_code - (4,161 unique values) - High-cardinality quasi-identifier\n",
        "3. ingest_marker - (Quasi-constant: 4,748 × 1.0 values) - Quasi-constant feature\n",
        "4. thermoreg_reading_fahrenheit - (Correlation with thermoreg_reading: 0.6836) - Scaled linear duplicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Characterize distributions**\n",
        "\n",
        "\t- For each numerical attribute, provide a detailed characterization of its value distribution. \n",
        "\t\t- Evaluate whether the distribution exhibits normality or skewness.\n",
        "\t\t- Determine if it is unimodal or multimodal.\n",
        "\t\t- Identify the presence of any outliers.\n",
        "\t\t- Justify your answers.\n",
        "\t- Create histograms to visually support your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "numeric_cols = ['thermoreg_reading', 'enzyme_activity_index', 'dual_lobe_signal', \n",
        "                'stress_variability', 'activity_score', 'fasting_flag', 'diagnostic_noise']\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    data = df[col].dropna()\n",
        "    \n",
        "    axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[i].set_title(f'{col}')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    \n",
        "    skewness = stats.skew(data)\n",
        "    kurtosis = stats.kurtosis(data)\n",
        "    \n",
        "    q1, q3 = data.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  Skewness: {skewness:.3f} ({'right' if skewness > 0.5 else 'left' if skewness < -0.5 else 'normal'})\")\n",
        "    print(f\"  Kurtosis: {kurtosis:.3f} ({'heavy-tailed' if kurtosis > 0 else 'light-tailed'})\")\n",
        "    print(f\"  Outliers: {len(outliers)} ({len(outliers)/len(data)*100:.1f}%)\")\n",
        "\n",
        "for j in range(len(numeric_cols), len(axes)):\n",
        "    axes[j].remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2.5\n",
        "\n",
        "\n",
        "- Computed skewness and kurtosis statistics for each numerical attribute\n",
        "- IQR-based outlier detection\n",
        "- Created histograms\n",
        "- Classified distributions based on statistical thresholds\n",
        "\n",
        "- thermoreg_reading: Left skewed (-2.102), heavy-tailed\n",
        "- enzyme_activity_index: Right skewed (1.839), heavy-tailed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **Class balance**\n",
        "\n",
        "\t- Report target proportions; include a simple bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_counts = df['health_outcome'].value_counts().sort_index()\n",
        "class_props = df['health_outcome'].value_counts(normalize=True).sort_index()\n",
        "\n",
        "print(\"Target class distribution:\")\n",
        "for class_val, count in class_counts.items():\n",
        "    prop = class_props[class_val]\n",
        "    print(f\"Class {class_val}: {count} samples ({prop:.1%})\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(class_counts.index, class_counts.values, color=['lightcoral', 'lightblue'], edgecolor='black')\n",
        "plt.xlabel('Health Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution')\n",
        "plt.xticks([0, 1], ['Unhealthy (0)', 'Healthy (1)'])\n",
        "\n",
        "for i, (bar, count) in enumerate(zip(bars, class_counts.values)):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 25, \n",
        "             str(count), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2.6\n",
        "\n",
        "1. Computed value counts and proportions for `health_outcome`\n",
        "2. Created a bar chart\n",
        "3. Evaluated balance\n",
        "\n",
        "- Class 0 (Unhealthy): 2,501 samples\n",
        "- Class 1 (Healthy): 2,499 samples\n",
        "- Difference: 2 samples\n",
        "\n",
        "The dataset has ideal class balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. **Save the clean data**\n",
        "\n",
        "\t- Keep the core features plus `health_outcome`.\n",
        "\t- Ensure correct dtypes (numeric/ordinal/binary).\n",
        "\t- Save as `alien_pet_health_cleaned.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Current data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nCleaned dataset shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "df['fasting_flag'] = df['fasting_flag'].fillna(0).astype('int8')\n",
        "\n",
        "core_features = ['thermoreg_reading', 'enzyme_activity_index', 'dual_lobe_signal', \n",
        "                'stress_variability', 'habitat_zone', 'activity_score', \n",
        "                'fasting_flag', 'calibration_tag', 'diagnostic_noise', 'health_outcome']\n",
        "\n",
        "df_final = df[core_features].copy()\n",
        "\n",
        "print(\"\\nFinal dataset info:\")\n",
        "print(f\"Shape: {df_final.shape}\")\n",
        "print(f\"Data types:\\n{df_final.dtypes}\")\n",
        "\n",
        "df_final.to_csv('alien_pet_health_cleaned.csv', index=False)\n",
        "print(\"\\nSaved as 'alien_pet_health_cleaned.csv'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}