{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "*Phase 2: Alien Pet Health, Machine Learning*\n",
        "\n",
        "# Project Context\n",
        "\n",
        "\n",
        "# Report: Alien Pet Health Machine Learning Pipeline\n",
        "\n",
        "This report documents the machine learning pipeline for the Alien Pet Health dataset. The analysis includes data loading, feature distribution analysis, preprocessing workflow design, model selection, hyperparameter tuning, and missing data imputation strategy.\n",
        "\n",
        "## 1. Data\n",
        "\n",
        "Phase 2 includes two versions of the same dataset: one containing missing values and the other without. The version with missing values was derived from the complete dataset:\n",
        "\n",
        "\n",
        "In your notebook, you can access and read the data directly from this GitHub repository.\n",
        "\n",
        "\n",
        "## 2. Tasks\n",
        "\n",
        "\n",
        "(1) **Load the dataset**\n",
        "\n",
        "- Read the CSV file without missing data (`alien_pet_health-realism-clean.csv`).\n",
        "- Show the shape of the data, as well as the first five rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Executive Summary - Key Results\n",
        "\n",
        "## Overview\n",
        "This report presents a complete machine learning pipeline for the Alien Pet Health dataset, including data preprocessing, model development, hyperparameter optimization, and missing data analysis.\n",
        "\n",
        "## Dataset Summary\n",
        "- **Size**: 5,000 samples × 8 columns (7 features + 1 target)\n",
        "- **Target**: Binary classification (health_outcome: 0=unhealthy, 1=healthy)\n",
        "- **Features**: 6 numerical, 1 categorical (habitat_zone with 5 categories)\n",
        "- **Class Balance**: Perfect balance (2,501 vs 2,499)\n",
        "- **Missing Data Version**: 3.31% missing values overall\n",
        "\n",
        "## Data Preprocessing\n",
        "- **Train-Test Split**: 80/20 stratified split (4,000 train, 1,000 test)\n",
        "- **Categorical Encoding**: OneHotEncoder with drop='first' (1 → 4 features)\n",
        "- **Numerical Scaling**: StandardScaler (mean ≈ 0, std ≈ 1)\n",
        "- **Final Features**: 10 total (6 numerical scaled + 4 categorical encoded)\n",
        "- **Data Leakage Prevention**: All transformers fitted on training data only\n",
        "\n",
        "## Model Performance Comparison\n",
        "\n",
        "### Default Parameters (Baseline)\n",
        "| Model | Train Accuracy | Test Accuracy | Test F1-Score |\n",
        "|-------|---------------|---------------|---------------|\n",
        "| Random Forest | 100.0% | 85.0% | 85.0% |\n",
        "| KNN | 88.9% | 83.9% | 83.9% |\n",
        "| Decision Tree | 100.0% | 79.5% | 79.5% |\n",
        "| Logistic Regression | 69.5% | 71.3% | 71.3% |\n",
        "\n",
        "### After Hyperparameter Optimization\n",
        "| Model | CV F1-Score | Test Accuracy | Test F1-Score | Best Parameters |\n",
        "|-------|-------------|---------------|---------------|-----------------|\n",
        "| **Random Forest** | **86.7% ± 0.002** | **85.0%** | **85.0%** | n_estimators=300, max_depth=15 |\n",
        "| KNN | 83.7% ± 0.004 | 85.1% | 85.0% | n_neighbors=21, weights='distance' |\n",
        "| Decision Tree | 81.9% ± 0.005 | 80.6% | 80.6% | max_depth=15, criterion='entropy' |\n",
        "| Logistic Regression | 68.8% ± 0.012 | 71.1% | 71.0% | penalty='l1', max_iter=100, solver='saga' |\n",
        "\n",
        "## Selected Model: Random Forest\n",
        "- **Justification**: Highest CV F1-score with excellent generalization\n",
        "- **CV Performance**: 86.7% ± 0.002 (5-fold cross-validation)\n",
        "- **Test Performance**: 85.0% F1-score, 85.0% accuracy\n",
        "- **CV-Test Consistency**: Excellent (+1.7% difference)\n",
        "- **Per-Class Performance**: 85% precision and 85% recall for both classes\n",
        "- **Configuration**: 300 trees, max depth 15\n",
        "\n",
        "## Missing Data Analysis\n",
        "- **Missing Rate**: 3.31% overall (1,324 missing values)\n",
        "- **Most Affected Feature**: fasting_flag (11.9% missing)\n",
        "- **Complete Cases**: 75.9% of rows\n",
        "- **Imputation Strategy**: Median (numerical), Most Frequent (categorical)\n",
        "- **Performance Impact**: \n",
        "  - Clean data: 86.7% CV F1, 85.0% test F1\n",
        "  - Imputed data: 84.5% CV F1, 84.0% test F1\n",
        "  - Degradation: 1.0% (acceptable for production use)\n",
        "- **Conclusion**: Simple imputation effective for low missing rate\n",
        "\n",
        "## Feature Importance (SHAP Analysis - Bonus)\n",
        "### Most Important Features (Across Models)\n",
        "1. **habitat_zone_c3**: Universally most important (rank 1.0)\n",
        "2. **activity_score**: Consistently ranked (average rank 3.5)\n",
        "3. **enzyme_activity_index**: Strong predictor (average rank 4.0)\n",
        "\n",
        "### Model-Specific Top Features\n",
        "- **KNN**: habitat_zone_c3 (0.145), stress_variability (0.132), enzyme_activity_index (0.116)\n",
        "- **Logistic Regression**: habitat_zone_c3 (0.683), activity_score (0.291), habitat_zone_c5 (0.229)\n",
        "\n",
        "### Key Insight\n",
        "Categorical habitat zones (especially c3) dominate feature importance, but numerical features show more consistent cross-model influence.\n",
        "\n",
        "## Key Findings and Recommendations\n",
        "\n",
        "### Model Selection\n",
        "- **Primary Recommendation**: Random Forest with 300 trees and max depth 15\n",
        "- **Reasoning**: Best CV performance, excellent generalization, robust ensemble method\n",
        "- **Alternative**: KNN (k=21, distance weights) for interpretability needs\n",
        "\n",
        "### Data Quality\n",
        "- Dataset is high quality with minimal missing values\n",
        "- Class balance eliminates bias concerns\n",
        "- Feature distributions support chosen preprocessing methods\n",
        "\n",
        "### Production Deployment\n",
        "- Model ready for deployment with 85% F1-score\n",
        "- Simple imputation strategy sufficient for handling missing data\n",
        "- Expected 1% performance degradation with real-world missing data\n",
        "- habitat_zone_c3 is critical feature for predictions\n",
        "\n",
        "### Model Characteristics\n",
        "- **Random Forest Strengths**: Handles feature interactions, resistant to overfitting, no assumptions about data distribution\n",
        "- **Preprocessing Requirements**: StandardScaler for numerical, OneHotEncoder for categorical\n",
        "- **Computational Cost**: Moderate (300 trees, 15 depth limit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Load the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load dataset\n",
        "url = \"data/alien_pet_health-realism-clean.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 1\n",
        "\n",
        "- Dataset loaded successfully from GitHub URL\n",
        "- Shape: 5,000 rows × 8 columns (7 features + 1 target)\n",
        "- Target variable: `health_outcome` (binary classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(2) **Feature Distribution Analysis**:\n",
        "\n",
        "- To identify the appropriate encoding method for each feature, it is helpful to examine their distributions using visualization tools such as histograms and box plots. This analysis will enable data-driven decisions on appropriate encoding strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Feature Distribution Analysis\n",
        "\n",
        "# Identify feature types\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Remove target from numerical features\n",
        "if 'health_outcome' in numerical_features:\n",
        "    numerical_features.remove('health_outcome')\n",
        "\n",
        "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(f\"Target variable: health_outcome\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Check unique values for categorical features\n",
        "print(\"\\nCategorical feature cardinality:\")\n",
        "for col in categorical_features:\n",
        "    n_unique = df[col].nunique()\n",
        "    print(f\"{col}: {n_unique} unique values\")\n",
        "    print(f\"  Values: {df[col].unique()}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Numerical feature distributions - Histograms\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "    data = df[col].dropna()\n",
        "    \n",
        "    axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n",
        "    axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
        "    axes[i].set_xlabel('Value')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].grid(alpha=0.3)\n",
        "    \n",
        "    # Compute statistics\n",
        "    skewness = stats.skew(data)\n",
        "    kurtosis = stats.kurtosis(data)\n",
        "    \n",
        "    # Add text box with statistics\n",
        "    textstr = f'Mean: {data.mean():.2f}\\nSkew: {skewness:.2f}\\nKurt: {kurtosis:.2f}'\n",
        "    axes[i].text(0.95, 0.95, textstr, transform=axes[i].transAxes, \n",
        "                fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(len(numerical_features), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Numerical Feature Distributions (Histograms)', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()\n",
        "\n",
        "# Numerical feature distributions - Box plots\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "    data = df[col].dropna()\n",
        "    \n",
        "    axes[i].boxplot(data, vert=True, patch_artist=True,\n",
        "                    boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
        "                    medianprops=dict(color='red', linewidth=2),\n",
        "                    whiskerprops=dict(color='black'),\n",
        "                    capprops=dict(color='black'))\n",
        "    axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
        "    axes[i].set_ylabel('Value')\n",
        "    axes[i].grid(alpha=0.3)\n",
        "    \n",
        "    # Outlier detection\n",
        "    q1, q3 = data.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "    \n",
        "    # Add text box with outlier count\n",
        "    textstr = f'Outliers: {len(outliers)}\\n({len(outliers)/len(data)*100:.1f}%)'\n",
        "    axes[i].text(0.95, 0.95, textstr, transform=axes[i].transAxes, \n",
        "                fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(len(numerical_features), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Numerical Feature Distributions (Box Plots)', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.show()\n",
        "\n",
        "# Categorical feature distributions\n",
        "if categorical_features:\n",
        "    n_cat = len(categorical_features)\n",
        "    fig, axes = plt.subplots(1, n_cat, figsize=(6*n_cat, 5))\n",
        "    if n_cat == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, col in enumerate(categorical_features):\n",
        "        counts = df[col].value_counts().sort_index()\n",
        "        \n",
        "        axes[i].bar(range(len(counts)), counts.values, color='coral', edgecolor='black', alpha=0.7)\n",
        "        axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
        "        axes[i].set_xlabel('Category')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        axes[i].set_xticks(range(len(counts)))\n",
        "        axes[i].set_xticklabels(counts.index, rotation=45, ha='right')\n",
        "        axes[i].grid(alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add count labels on bars\n",
        "        for j, (idx, val) in enumerate(counts.items()):\n",
        "            axes[i].text(j, val + max(counts.values)*0.01, str(val), \n",
        "                        ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Categorical Feature Distributions', fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nNumerical feature summary statistics:\")\n",
        "print(df[numerical_features].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 2\n",
        "\n",
        "### Results and Analysis - Task 2\n",
        "\n",
        "**Feature Types:**\n",
        "- 1 categorical: `habitat_zone` (5 categories: c1, c2, c3, c4, c5)\n",
        "- 6 numerical: All show near-normal distributions with minimal skewness\n",
        "\n",
        "**Distribution Analysis:**\n",
        "- Most numerical features: Normal distributions (|skew| < 0.3)\n",
        "- `activity_score`: Discrete uniform (values 1-5)\n",
        "- `fasting_flag`: Binary (40% ones, 60% zeros)\n",
        "- Minimal outliers detected\n",
        "\n",
        "**Encoding Strategy:**\n",
        "- `habitat_zone`: OneHotEncoder (nominal categories, no ordering)\n",
        "- Numerical features: StandardScaler (different scales, normal distributions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(3) **Training and Target Data**:\n",
        "\n",
        "- For each dataset, define Python variables, such as `X` for the data and `y` for the target class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Training and Target Data\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('health_outcome', axis=1)\n",
        "y = df['health_outcome']\n",
        "\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
        "print(f\"\\nTarget variable distribution:\")\n",
        "print(y.value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(4) **Data Splitting**:\n",
        "\n",
        "- Split the dataset into training (80%) and test (20%) sets using the holdout method.\n",
        "\n",
        "- Ensure that this split occurs before any preprocessing to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Data Splitting\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train (80%) and test (20%) sets\n",
        "# This occurs BEFORE any preprocessing to prevent data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # Ensures balanced split across target classes\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\nTest target distribution:\")\n",
        "print(y_test.value_counts().sort_index())\n",
        "\n",
        "# Verify proportions\n",
        "print(f\"\\nTraining set proportion: {len(X_train) / len(X):.1%}\")\n",
        "print(f\"Test set proportion: {len(X_test) / len(X):.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Tasks 3 & 4\n",
        "\n",
        "**Task 3: Training and Target Data**\n",
        "- Features (X): 7 features (6 numerical, 1 categorical)\n",
        "- Target (y): Binary classification (0=unhealthy, 1=healthy)\n",
        "- Perfect class balance: 2,501 vs 2,499 samples\n",
        "\n",
        "**Task 4: Data Splitting**\n",
        "- 80/20 train-test split with stratification\n",
        "- Training: 4,000 samples (2,001 vs 1,999)\n",
        "- Test: 1,000 samples (500 vs 500)\n",
        "- Class balance preserved in both sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Pre-Processing\n",
        "\n",
        "(5) **Categorical Variable Encoding**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5: Categorical Variable Encoding\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = ['habitat_zone']\n",
        "numerical_features = ['thermoreg_reading', 'enzyme_activity_index', 'dual_lobe_signal', \n",
        "                     'stress_variability', 'activity_score', 'fasting_flag']\n",
        "\n",
        "print(\"Feature separation:\")\n",
        "print(f\"Categorical features: {categorical_features}\")\n",
        "print(f\"Numerical features: {numerical_features}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "# drop='first' removes one column to avoid multicollinearity\n",
        "# sparse_output=False returns dense array for easier handling\n",
        "categorical_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "\n",
        "# Fit encoder on training data only (prevents data leakage)\n",
        "X_train_categorical = X_train[categorical_features]\n",
        "X_test_categorical = X_test[categorical_features]\n",
        "\n",
        "print(\"Original categorical feature values:\")\n",
        "print(\"Training set habitat_zone distribution:\")\n",
        "print(X_train_categorical['habitat_zone'].value_counts().sort_index())\n",
        "print(\"\\nTest set habitat_zone distribution:\")\n",
        "print(X_test_categorical['habitat_zone'].value_counts().sort_index())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_categorical_encoded = categorical_encoder.fit_transform(X_train_categorical)\n",
        "\n",
        "# Transform test data using the fitted encoder (no fitting on test data)\n",
        "X_test_categorical_encoded = categorical_encoder.transform(X_test_categorical)\n",
        "\n",
        "# Get feature names for encoded categorical variables\n",
        "categorical_feature_names = categorical_encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "print(\"Encoding results:\")\n",
        "print(f\"Original categorical features: {len(categorical_features)}\")\n",
        "print(f\"Encoded categorical features: {len(categorical_feature_names)}\")\n",
        "print(f\"New feature names: {list(categorical_feature_names)}\")\n",
        "print(f\"Training encoded shape: {X_train_categorical_encoded.shape}\")\n",
        "print(f\"Test encoded shape: {X_test_categorical_encoded.shape}\")\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "X_train_categorical_encoded_df = pd.DataFrame(\n",
        "    X_train_categorical_encoded, \n",
        "    columns=categorical_feature_names,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_categorical_encoded_df = pd.DataFrame(\n",
        "    X_test_categorical_encoded, \n",
        "    columns=categorical_feature_names,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nSample of encoded training data:\")\n",
        "print(X_train_categorical_encoded_df.head())\n",
        "print(f\"\\nEncoded feature value counts (training):\")\n",
        "for col in categorical_feature_names:\n",
        "    print(f\"{col}: {X_train_categorical_encoded_df[col].sum()} ones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(6) **Normalization/Standardization of Numerical Features**:\n",
        "\n",
        "- Normalize or standardize numerical features if necessary. Describe the technique used (e.g., Min-Max scaling, StandardScaler) and explain why it is suitable for this dataset.\n",
        "\n",
        "- Ensure that this technique is applied only to the training data, with the same transformation subsequently applied to the test data without fitting on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 6: Normalization/Standardization of Numerical Features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Extract numerical features from training and test sets\n",
        "X_train_numerical = X_train[numerical_features]\n",
        "X_test_numerical = X_test[numerical_features]\n",
        "\n",
        "print(\"Original numerical features statistics (training set):\")\n",
        "print(X_train_numerical.describe())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "numerical_scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler on training data only (prevents data leakage)\n",
        "X_train_numerical_scaled = numerical_scaler.fit_transform(X_train_numerical)\n",
        "\n",
        "# Transform test data using the fitted scaler (no fitting on test data)\n",
        "X_test_numerical_scaled = numerical_scaler.transform(X_test_numerical)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_numerical_scaled_df = pd.DataFrame(\n",
        "    X_train_numerical_scaled, \n",
        "    columns=numerical_features,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_numerical_scaled_df = pd.DataFrame(\n",
        "    X_test_numerical_scaled, \n",
        "    columns=numerical_features,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(\"Scaled numerical features statistics (training set):\")\n",
        "print(X_train_numerical_scaled_df.describe())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"Scaling verification:\")\n",
        "print(\"Training set means (should be ~0):\")\n",
        "print(X_train_numerical_scaled_df.mean())\n",
        "print(\"\\nTraining set standard deviations (should be ~1):\")\n",
        "print(X_train_numerical_scaled_df.std())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical features\n",
        "X_train_preprocessed = pd.concat([\n",
        "    X_train_numerical_scaled_df, \n",
        "    X_train_categorical_encoded_df\n",
        "], axis=1)\n",
        "\n",
        "X_test_preprocessed = pd.concat([\n",
        "    X_test_numerical_scaled_df, \n",
        "    X_test_categorical_encoded_df\n",
        "], axis=1)\n",
        "\n",
        "print(\"Final preprocessed dataset:\")\n",
        "print(f\"Training set shape: {X_train_preprocessed.shape}\")\n",
        "print(f\"Test set shape: {X_test_preprocessed.shape}\")\n",
        "print(f\"Feature names: {list(X_train_preprocessed.columns)}\")\n",
        "print(f\"\\nFirst 5 rows of preprocessed training data:\")\n",
        "print(X_train_preprocessed.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Tasks 5 & 6\n",
        "\n",
        "**Task 5: Categorical Encoding**\n",
        "- OneHotEncoder with `drop='first'` applied to `habitat_zone`\n",
        "- 1 categorical → 4 binary features (c1 as reference)\n",
        "- No data leakage: fitted on training data only\n",
        "\n",
        "**Task 6: Numerical Scaling**\n",
        "- StandardScaler applied to 6 numerical features\n",
        "- Achieved mean ≈ 0, std ≈ 1 for all features\n",
        "- Final dataset: 10 features (6 scaled + 4 encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Development & Evaluation\n",
        "\n",
        "(7) **Model Development**:\n",
        "\n",
        "- Implement the machine learning models covered in class: K-Nearest Neighbors (KNN), Decision Trees, and Logistic Regression, as well as Random Forest. Use the default parameters of scikit-learn as a baseline for training each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 7: Model Development\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Initialize models with default parameters\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "print(\"Model initialization with default parameters:\")\n",
        "print(\"=\"*60)\n",
        "for name, model in models.items():\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Parameters: {model.get_params()}\")\n",
        "    print()\n",
        "\n",
        "# Train each model and evaluate on training data (baseline performance)\n",
        "print(\"Training models and baseline evaluation:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trained_models = {}\n",
        "baseline_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train_preprocessed, y_train)\n",
        "    trained_models[name] = model\n",
        "    \n",
        "    # Predict on training data (for baseline comparison)\n",
        "    y_train_pred = model.predict(X_train_preprocessed)\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    \n",
        "    # Predict on test data (holdout evaluation)\n",
        "    y_test_pred = model.predict(X_test_preprocessed)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    \n",
        "    baseline_results[name] = {\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'y_test_pred': y_test_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"  Training accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Baseline Performance Summary:\")\n",
        "print(f\"{'Model':<20} {'Train Acc':<12} {'Test Acc':<12} {'Difference':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for name, results in baseline_results.items():\n",
        "    diff = results['train_accuracy'] - results['test_accuracy']\n",
        "    print(f\"{name:<20} {results['train_accuracy']:<12.4f} {results['test_accuracy']:<12.4f} {diff:<12.4f}\")\n",
        "\n",
        "# Detailed classification report for each model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Detailed Classification Reports (Test Set):\")\n",
        "for name, results in baseline_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(classification_report(y_test, results['y_test_pred'], target_names=['Unhealthy', 'Healthy']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(8) **Model Evaluation**:\n",
        "\n",
        "- Use cross-validation to evaluate each model, justifying your choice of the number of folds.\n",
        "\n",
        "- Assess the models using metrics such as precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 8: Model Evaluation with Cross-Validation\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# Cross-validation parameters\n",
        "cv_folds = 5  # 5-fold cross-validation\n",
        "\n",
        "print(\"Cross-Validation Evaluation:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Number of folds: {cv_folds}\")\n",
        "print(f\"Justification: 5-fold CV provides good balance between:\")\n",
        "print(f\"  - Bias-variance tradeoff (not too high bias like 3-fold)\")\n",
        "print(f\"  - Computational efficiency (not too expensive like 10-fold)\")\n",
        "print(f\"  - Sufficient training data per fold (4000 * 4/5 = 3200 samples)\")\n",
        "print(f\"  - Reliable performance estimates with reasonable variance\")\n",
        "print()\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluating {name} with {cv_folds}-fold CV...\")\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_scores = cross_validate(\n",
        "        model, \n",
        "        X_train_preprocessed, \n",
        "        y_train,\n",
        "        cv=cv_folds,\n",
        "        scoring=scoring,\n",
        "        return_train_score=True,\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "    \n",
        "    # Calculate mean and std for each metric\n",
        "    results = {}\n",
        "    for metric in scoring.keys():\n",
        "        test_scores = cv_scores[f'test_{metric}']\n",
        "        train_scores = cv_scores[f'train_{metric}']\n",
        "        \n",
        "        results[f'{metric}_test_mean'] = np.mean(test_scores)\n",
        "        results[f'{metric}_test_std'] = np.std(test_scores)\n",
        "        results[f'{metric}_train_mean'] = np.mean(train_scores)\n",
        "        results[f'{metric}_train_std'] = np.std(train_scores)\n",
        "        results[f'{metric}_test_scores'] = test_scores\n",
        "        results[f'{metric}_train_scores'] = train_scores\n",
        "    \n",
        "    cv_results[name] = results\n",
        "\n",
        "print(\"\\nCross-Validation Results Summary:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Model':<18} {'Metric':<12} {'CV Mean ± Std':<20} {'Train Mean ± Std':<20}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name in models.keys():\n",
        "    results = cv_results[name]\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        cv_mean = results[f'{metric}_test_mean']\n",
        "        cv_std = results[f'{metric}_test_std']\n",
        "        train_mean = results[f'{metric}_train_mean']\n",
        "        train_std = results[f'{metric}_train_std']\n",
        "        \n",
        "        metric_display = metric.capitalize()\n",
        "        if name != list(models.keys())[0] or metric != 'accuracy':\n",
        "            name_display = \"\" if metric != 'accuracy' else name\n",
        "        else:\n",
        "            name_display = name\n",
        "            \n",
        "        print(f\"{name_display:<18} {metric_display:<12} {cv_mean:.4f} ± {cv_std:.4f}     {train_mean:.4f} ± {train_std:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Detailed fold-by-fold results\n",
        "print(\"\\nDetailed Fold-by-Fold Results:\")\n",
        "print(\"=\"*60)\n",
        "for name, results in cv_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Fold-by-fold CV scores:\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        scores = results[f'{metric}_test_scores']\n",
        "        scores_str = \" \".join([f\"{score:.4f}\" for score in scores])\n",
        "        print(f\"    {metric.capitalize()}: [{scores_str}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Tasks 7 & 8\n",
        "\n",
        "**Task 7: Model Development**\n",
        "- 4 models implemented: KNN, Decision Tree, Logistic Regression, Random Forest\n",
        "- All used default scikit-learn parameters\n",
        "- Baseline test performance: Random Forest (85.0%) > KNN (83.9%) > Decision Tree (79.5%) > Logistic Regression (71.3%)\n",
        "\n",
        "**Task 8: Cross-Validation Evaluation**\n",
        "- 5-fold stratified CV with accuracy, precision, recall, F1-score\n",
        "- Random Forest: Most stable (lowest CV variance)\n",
        "- Decision Tree: Severe overfitting (100% train → 79.5% test)\n",
        "- Logistic Regression: Underfitting (69.5% train → 71.3% test)\n",
        "- **Logistic Regression**: Negative gap suggests underfitting (model too simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Optimization\n",
        "\n",
        "(9) **Exploration and Performance Evaluation:**\n",
        "\n",
        "- Investigate the impact of varying hyperparameter values on the performance of each model.\n",
        "\n",
        "- For each model, ensure to vary at least the following hyperparameters:\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "- Employ a grid search strategy or utilize scikit-learn's built-in methods to thoroughly evaluate all combinations of hyperparameter values. Cross-validation should be used to assess each combination.\n",
        "\n",
        "- Quantify the performance of each hyperparameter configuration using precision, recall, and F1-score as metrics. Report both the mean and standard deviation.\n",
        "\n",
        "- Display the results in a tabular or graphical format (e.g., line charts, bar charts) to effectively demonstrate the influence of hyperparameter variations on model performance.\n",
        "\n",
        "- Specify the default values for each hyperparameter tested.\n",
        "\n",
        "- Analyze the findings and offer insights into which hyperparameter configurations achieved optimal performance for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 9: Hyperparameter Optimization\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9, 11, 15, 21],\n",
        "        'weights': ['uniform', 'distance']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [3, 5, 7, 10, 15, 20, None]\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
        "        'max_iter': [100, 500, 1000, 2000],\n",
        "        'tol': [1e-4, 1e-3, 1e-2]\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'max_depth': [3, 5, 10, 15, 20, None]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter Grid Search\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display default values and search spaces\n",
        "print(\"Default values and search spaces:\")\n",
        "for model_name, grid in param_grids.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    model = models[model_name]\n",
        "    for param, values in grid.items():\n",
        "        default_val = model.get_params()[param]\n",
        "        print(f\"  {param}: default={default_val}, search={values}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Performing Grid Search with 5-fold CV...\")\n",
        "\n",
        "# Perform grid search for each model\n",
        "grid_search_results = {}\n",
        "best_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nOptimizing {model_name}...\")\n",
        "    \n",
        "    # Special handling for Logistic Regression solver compatibility\n",
        "    if model_name == 'Logistic Regression':\n",
        "        # Need different solvers for different penalties\n",
        "        param_grid_expanded = []\n",
        "        base_grid = param_grids[model_name]\n",
        "        \n",
        "        for penalty in base_grid['penalty']:\n",
        "            for max_iter in base_grid['max_iter']:\n",
        "                for tol in base_grid['tol']:\n",
        "                    if penalty == 'l1':\n",
        "                        solvers = ['liblinear', 'saga']\n",
        "                    elif penalty == 'elasticnet':\n",
        "                        solvers = ['saga']\n",
        "                        # Add l1_ratio for elasticnet\n",
        "                        param_grid_expanded.extend([\n",
        "                            {'penalty': [penalty], 'max_iter': [max_iter], 'tol': [tol], \n",
        "                             'solver': [solver], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
        "                            for solver in solvers\n",
        "                        ])\n",
        "                        continue\n",
        "                    elif penalty is None:\n",
        "                        solvers = ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
        "                    else:  # l2\n",
        "                        solvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n",
        "                    \n",
        "                    param_grid_expanded.extend([\n",
        "                        {'penalty': [penalty], 'max_iter': [max_iter], 'tol': [tol], 'solver': [solver]}\n",
        "                        for solver in solvers\n",
        "                    ])\n",
        "        \n",
        "        param_grid = param_grid_expanded\n",
        "    else:\n",
        "        param_grid = param_grids[model_name]\n",
        "    \n",
        "    # Perform grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='f1_weighted',\n",
        "        n_jobs=-1,\n",
        "        return_train_score=True\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_preprocessed, y_train)\n",
        "    \n",
        "    # Store results\n",
        "    grid_search_results[model_name] = grid_search\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"  Best CV F1-score: {grid_search.best_score_:.4f}\")\n",
        "    print(f\"  Default F1-score: {cv_results[model_name]['f1_test_mean']:.4f}\")\n",
        "    improvement = grid_search.best_score_ - cv_results[model_name]['f1_test_mean']\n",
        "    print(f\"  Improvement: {improvement:+.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Hyperparameter Optimization Summary:\")\n",
        "print(f\"{'Model':<18} {'Default F1':<12} {'Best F1':<12} {'Improvement':<12} {'Best Parameters'}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for model_name in models.keys():\n",
        "    default_f1 = cv_results[model_name]['f1_test_mean']\n",
        "    best_f1 = grid_search_results[model_name].best_score_\n",
        "    improvement = best_f1 - default_f1\n",
        "    best_params = str(grid_search_results[model_name].best_params_)[:50] + \"...\"\n",
        "    \n",
        "    print(f\"{model_name:<18} {default_f1:<12.4f} {best_f1:<12.4f} {improvement:<+12.4f} {best_params}\")\n",
        "\n",
        "# Detailed results for each model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Detailed Hyperparameter Analysis:\")\n",
        "\n",
        "for model_name, grid_search in grid_search_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Total combinations tested: {len(grid_search.cv_results_['params'])}\")\n",
        "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"  Best CV score: {grid_search.best_score_:.4f} ± {grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")\n",
        "    \n",
        "    # Top 3 configurations\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    top_3 = results_df.nlargest(3, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
        "    \n",
        "    print(f\"  Top 3 configurations:\")\n",
        "    for i, (idx, row) in enumerate(top_3.iterrows(), 1):\n",
        "        params_str = str(row['params'])[:60] + \"...\" if len(str(row['params'])) > 60 else str(row['params'])\n",
        "        print(f\"    {i}. {row['mean_test_score']:.4f} ± {row['std_test_score']:.4f} | {params_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of Results\n",
        "\n",
        "(10) **Model Comparison**:\n",
        "\n",
        "- Compare the results obtained from each model.\n",
        "\n",
        "- Discuss observed differences in model performance, providing potential explanations. Consider aspects such as model complexity, data imbalance, overfitting, and the impact of parameter tuning on overall results.\n",
        "\n",
        "- Provide recommendations on which model(s) to choose for this task and justify your choices based on the analysis results.\n",
        "\n",
        "- Train the recommended model(s) using the optimal parameter values identified from the parameter optimization step. Subsequently, apply the trained model to the test data. Document your observations comprehensively. Specifically, evaluate whether the results derived from cross-validation are consistent with those obtained from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 10: Model Comparison and Final Evaluation\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL COMPARISON AND SELECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare optimized models\n",
        "final_results = {}\n",
        "\n",
        "print(\"\\nOptimized Model Performance Comparison:\")\n",
        "print(f\"{'Model':<18} {'CV F1-Score':<15} {'Test Accuracy':<15} {'Test F1-Score':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for model_name, best_model in best_models.items():\n",
        "    # Cross-validation score (already computed)\n",
        "    cv_f1 = grid_search_results[model_name].best_score_\n",
        "    cv_std = grid_search_results[model_name].cv_results_['std_test_score'][grid_search_results[model_name].best_index_]\n",
        "    \n",
        "    # Test set evaluation\n",
        "    y_test_pred = best_model.predict(X_test_preprocessed)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "    \n",
        "    final_results[model_name] = {\n",
        "        'cv_f1_mean': cv_f1,\n",
        "        'cv_f1_std': cv_std,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_f1': test_f1,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'model': best_model\n",
        "    }\n",
        "    \n",
        "    print(f\"{model_name:<18} {cv_f1:.4f} ± {cv_std:.4f}   {test_accuracy:<15.4f} {test_f1:<15.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(final_results.keys(), key=lambda x: final_results[x]['cv_f1_mean'])\n",
        "best_model_obj = final_results[best_model_name]['model']\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"MODEL SELECTION DECISION:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Recommended Model: {best_model_name}\")\n",
        "print(f\"CV F1-Score: {final_results[best_model_name]['cv_f1_mean']:.4f} ± {final_results[best_model_name]['cv_f1_std']:.4f}\")\n",
        "print(f\"Test F1-Score: {final_results[best_model_name]['test_f1']:.4f}\")\n",
        "print(f\"Test Accuracy: {final_results[best_model_name]['test_accuracy']:.4f}\")\n",
        "\n",
        "# Consistency analysis\n",
        "cv_test_diff = final_results[best_model_name]['cv_f1_mean'] - final_results[best_model_name]['test_f1']\n",
        "print(f\"\\nCV vs Test Consistency:\")\n",
        "print(f\"F1-Score difference (CV - Test): {cv_test_diff:+.4f}\")\n",
        "if abs(cv_test_diff) < 0.02:\n",
        "    consistency = \"Excellent\"\n",
        "elif abs(cv_test_diff) < 0.05:\n",
        "    consistency = \"Good\"\n",
        "else:\n",
        "    consistency = \"Poor\"\n",
        "print(f\"Consistency assessment: {consistency}\")\n",
        "\n",
        "print(f\"\\nBest Model Parameters:\")\n",
        "for param, value in best_model_obj.get_params().items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Detailed classification report for best model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"DETAILED EVALUATION - {best_model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(classification_report(y_test, final_results[best_model_name]['y_test_pred'], \n",
        "                          target_names=['Unhealthy', 'Healthy']))\n",
        "\n",
        "# Performance comparison analysis\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"PERFORMANCE ANALYSIS BY MODEL:\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for model_name, results in final_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Strengths:\")\n",
        "    \n",
        "    # Analyze each model's characteristics\n",
        "    if model_name == 'KNN':\n",
        "        print(f\"    - Instance-based learning, good for local patterns\")\n",
        "        print(f\"    - Non-parametric, makes no assumptions about data distribution\")\n",
        "        best_k = best_models[model_name].get_params()['n_neighbors']\n",
        "        print(f\"    - Optimal k={best_k} balances bias-variance tradeoff\")\n",
        "    \n",
        "    elif model_name == 'Decision Tree':\n",
        "        print(f\"    - Highly interpretable, rule-based decisions\")\n",
        "        print(f\"    - Handles both numerical and categorical features naturally\")\n",
        "        max_depth = best_models[model_name].get_params()['max_depth']\n",
        "        print(f\"    - Optimal max_depth={max_depth} controls overfitting\")\n",
        "    \n",
        "    elif model_name == 'Logistic Regression':\n",
        "        print(f\"    - Linear decision boundary, computationally efficient\")\n",
        "        print(f\"    - Provides probability estimates for predictions\")\n",
        "        penalty = best_models[model_name].get_params()['penalty']\n",
        "        print(f\"    - Regularization ({penalty}) prevents overfitting\")\n",
        "    \n",
        "    elif model_name == 'Random Forest':\n",
        "        print(f\"    - Ensemble method reduces overfitting\")\n",
        "        print(f\"    - Handles feature interactions and non-linearities\")\n",
        "        n_est = best_models[model_name].get_params()['n_estimators']\n",
        "        print(f\"    - {n_est} trees provide robust predictions\")\n",
        "    \n",
        "    print(f\"  Performance: CV F1={results['cv_f1_mean']:.4f}, Test F1={results['test_f1']:.4f}\")\n",
        "    \n",
        "    # Identify potential issues\n",
        "    cv_test_gap = abs(results['cv_f1_mean'] - results['test_f1'])\n",
        "    if cv_test_gap > 0.05:\n",
        "        print(f\"  Warning: Large CV-Test gap ({cv_test_gap:.4f}) suggests overfitting\")\n",
        "    elif results['cv_f1_mean'] < 0.80:\n",
        "        print(f\"  Warning: Low performance may indicate underfitting\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL RECOMMENDATIONS:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"1. Primary Model: {best_model_name}\")\n",
        "print(f\"   - Highest CV performance with good generalization\")\n",
        "print(f\"   - Recommended for production use\")\n",
        "print(f\"\\n2. Alternative Models:\")\n",
        "\n",
        "# Sort other models by performance\n",
        "other_models = sorted([(name, results['cv_f1_mean']) for name, results in final_results.items() \n",
        "                      if name != best_model_name], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for i, (name, score) in enumerate(other_models, 2):\n",
        "    print(f\"   {chr(ord('a') + i - 2)}. {name} (CV F1: {score:.4f})\")\n",
        "\n",
        "print(f\"\\n3. Model Selection Criteria:\")\n",
        "print(f\"   - Cross-validation F1-score (primary metric)\")\n",
        "print(f\"   - CV-Test consistency (generalization)\")\n",
        "print(f\"   - Model interpretability and complexity\")\n",
        "print(f\"   - Computational efficiency considerations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Tasks 9 & 10\n",
        "\n",
        "**Task 9: Hyperparameter Optimization**\n",
        "- Grid search with 5-fold CV using F1-weighted score\n",
        "- Best parameters found: Random Forest (300 trees, depth=15), KNN (k=21, distance weights), Decision Tree (depth=15, gini)\n",
        "- Performance improvements: Random Forest (+1.7%), KNN (+0.2%), Decision Tree (+2.4%)\n",
        "\n",
        "**Task 10: Final Model Selection**  \n",
        "**Winner: Random Forest**\n",
        "- CV F1: 86.7% ± 0.0016 (best and most stable)\n",
        "- Test F1: 85.0% (excellent CV-test consistency: +1.7%)\n",
        "- Balanced performance: 85% precision and recall for both classes\n",
        "- Robust ensemble with controlled overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "(11) **Evaluate how missing values affect model performance:**\n",
        "\n",
        "- Read the CSV file with missing data (`alien_pet_health-realism-clean-missing.csv`).\n",
        "\n",
        "- Present a brief analysis of missing data within the dataset by reporting both the count and percentage of missing values foreach column as well as for the dataset as a whole. Additionally, provide a breakdown of the number and proportion of rowscategorized by the absence of missing data, and those containing one, two, or more missing values.\n",
        "\n",
        "- Apply a simple imputation strategy (for instance, median for numeric, and most_frequent for categoricals).\n",
        "\n",
        "- Standardize and normalize the numerical features, and encode the categorical data using the data pre-processing methodspreviously described.\n",
        "\n",
        "- Utilize cross-validation to assess the effectiveness of the data imputation strategy, given that the optimal combination oflearning algorithms and hyperparameters has already been determined.\n",
        "\n",
        "- Discuss observed differences in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 11: Missing Data Analysis\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"TASK 11: MISSING DATA ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load dataset with missing values\n",
        "missing_url = \"data/alien_pet_health-realism-clean-missing.csv\"\n",
        "df_missing = pd.read_csv(missing_url)\n",
        "\n",
        "print(f\"Dataset with missing values shape: {df_missing.shape}\")\n",
        "print(f\"Original clean dataset shape: {df.shape}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Missing data analysis\n",
        "print(\"MISSING DATA ANALYSIS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count and percentage of missing values per column\n",
        "missing_counts = df_missing.isnull().sum()\n",
        "missing_percentages = (df_missing.isnull().sum() / len(df_missing)) * 100\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Count': missing_counts,\n",
        "    'Missing %': missing_percentages\n",
        "})\n",
        "print(missing_summary)\n",
        "print()\n",
        "\n",
        "# Overall dataset missing analysis\n",
        "total_cells = df_missing.shape[0] * df_missing.shape[1]\n",
        "total_missing = df_missing.isnull().sum().sum()\n",
        "overall_missing_pct = (total_missing / total_cells) * 100\n",
        "\n",
        "print(f\"Total cells in dataset: {total_cells:,}\")\n",
        "print(f\"Total missing values: {total_missing:,}\")\n",
        "print(f\"Overall missing percentage: {overall_missing_pct:.2f}%\")\n",
        "print()\n",
        "\n",
        "# Row-wise missing value analysis\n",
        "rows_missing_count = df_missing.isnull().sum(axis=1)\n",
        "missing_row_breakdown = rows_missing_count.value_counts().sort_index()\n",
        "\n",
        "print(\"Breakdown of rows by number of missing values:\")\n",
        "for num_missing, count in missing_row_breakdown.items():\n",
        "    percentage = (count / len(df_missing)) * 100\n",
        "    if num_missing == 0:\n",
        "        print(f\"  {num_missing} missing values: {count:,} rows ({percentage:.1f}%) - Complete cases\")\n",
        "    else:\n",
        "        print(f\"  {num_missing} missing values: {count:,} rows ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPUTATION STRATEGY:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and target\n",
        "X_missing = df_missing.drop('health_outcome', axis=1)\n",
        "y_missing = df_missing['health_outcome']\n",
        "\n",
        "print(f\"Features with missing values: {X_missing.columns[X_missing.isnull().any()].tolist()}\")\n",
        "print(f\"Target variable missing values: {y_missing.isnull().sum()}\")\n",
        "\n",
        "# Split the missing data (same random state for consistency)\n",
        "X_train_missing, X_test_missing, y_train_missing, y_test_missing = train_test_split(\n",
        "    X_missing, y_missing, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_missing\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set missing data shape: {X_train_missing.shape}\")\n",
        "print(f\"Test set missing data shape: {X_test_missing.shape}\")\n",
        "\n",
        "# Apply imputation strategy\n",
        "print(\"\\nApplying imputation strategy:\")\n",
        "print(\"- Numerical features: Median imputation\")\n",
        "print(\"- Categorical features: Most frequent imputation\")\n",
        "\n",
        "# Separate numerical and categorical features\n",
        "categorical_features = ['habitat_zone']\n",
        "numerical_features = ['thermoreg_reading', 'enzyme_activity_index', 'dual_lobe_signal', \n",
        "                     'stress_variability', 'activity_score', 'fasting_flag']\n",
        "\n",
        "# Initialize imputers\n",
        "numerical_imputer = SimpleImputer(strategy='median')\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Impute training data\n",
        "X_train_missing_num_imputed = numerical_imputer.fit_transform(X_train_missing[numerical_features])\n",
        "X_train_missing_cat_imputed = categorical_imputer.fit_transform(X_train_missing[categorical_features])\n",
        "\n",
        "# Impute test data using training fitted imputers\n",
        "X_test_missing_num_imputed = numerical_imputer.transform(X_test_missing[numerical_features])\n",
        "X_test_missing_cat_imputed = categorical_imputer.transform(X_test_missing[categorical_features])\n",
        "\n",
        "# Convert back to DataFrames\n",
        "X_train_missing_num_df = pd.DataFrame(X_train_missing_num_imputed, columns=numerical_features, index=X_train_missing.index)\n",
        "X_train_missing_cat_df = pd.DataFrame(X_train_missing_cat_imputed, columns=categorical_features, index=X_train_missing.index)\n",
        "\n",
        "X_test_missing_num_df = pd.DataFrame(X_test_missing_num_imputed, columns=numerical_features, index=X_test_missing.index)\n",
        "X_test_missing_cat_df = pd.DataFrame(X_test_missing_cat_imputed, columns=categorical_features, index=X_test_missing.index)\n",
        "\n",
        "# Combine numerical and categorical\n",
        "X_train_missing_imputed = pd.concat([X_train_missing_num_df, X_train_missing_cat_df], axis=1)\n",
        "X_test_missing_imputed = pd.concat([X_test_missing_num_df, X_test_missing_cat_df], axis=1)\n",
        "\n",
        "print(f\"\\nImputed training set shape: {X_train_missing_imputed.shape}\")\n",
        "print(f\"Imputed test set shape: {X_test_missing_imputed.shape}\")\n",
        "print(f\"Missing values after imputation (train): {X_train_missing_imputed.isnull().sum().sum()}\")\n",
        "print(f\"Missing values after imputation (test): {X_test_missing_imputed.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING IMPUTED DATA:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Apply the same preprocessing as before (encoding and scaling)\n",
        "# Use the SAME encoders and scalers fitted on the original clean data\n",
        "\n",
        "# Categorical encoding using the same encoder\n",
        "X_train_missing_cat_encoded = categorical_encoder.transform(X_train_missing_imputed[categorical_features])\n",
        "X_test_missing_cat_encoded = categorical_encoder.transform(X_test_missing_imputed[categorical_features])\n",
        "\n",
        "# Numerical scaling using the same scaler\n",
        "X_train_missing_num_scaled = numerical_scaler.transform(X_train_missing_imputed[numerical_features])\n",
        "X_test_missing_num_scaled = numerical_scaler.transform(X_test_missing_imputed[numerical_features])\n",
        "\n",
        "# Convert to DataFrames\n",
        "X_train_missing_cat_encoded_df = pd.DataFrame(\n",
        "    X_train_missing_cat_encoded, \n",
        "    columns=categorical_feature_names,\n",
        "    index=X_train_missing_imputed.index\n",
        ")\n",
        "\n",
        "X_test_missing_cat_encoded_df = pd.DataFrame(\n",
        "    X_test_missing_cat_encoded, \n",
        "    columns=categorical_feature_names,\n",
        "    index=X_test_missing_imputed.index\n",
        ")\n",
        "\n",
        "X_train_missing_num_scaled_df = pd.DataFrame(\n",
        "    X_train_missing_num_scaled, \n",
        "    columns=numerical_features,\n",
        "    index=X_train_missing_imputed.index\n",
        ")\n",
        "\n",
        "X_test_missing_num_scaled_df = pd.DataFrame(\n",
        "    X_test_missing_num_scaled, \n",
        "    columns=numerical_features,\n",
        "    index=X_test_missing_imputed.index\n",
        ")\n",
        "\n",
        "# Combine final preprocessed data\n",
        "X_train_missing_preprocessed = pd.concat([\n",
        "    X_train_missing_num_scaled_df, \n",
        "    X_train_missing_cat_encoded_df\n",
        "], axis=1)\n",
        "\n",
        "X_test_missing_preprocessed = pd.concat([\n",
        "    X_test_missing_num_scaled_df, \n",
        "    X_test_missing_cat_encoded_df\n",
        "], axis=1)\n",
        "\n",
        "print(f\"Final preprocessed missing data shape (train): {X_train_missing_preprocessed.shape}\")\n",
        "print(f\"Final preprocessed missing data shape (test): {X_test_missing_preprocessed.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL EVALUATION ON IMPUTED DATA:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the best model (Random Forest) with optimal parameters from Task 10\n",
        "best_rf_model = best_models['Random Forest']\n",
        "\n",
        "# Evaluate on imputed data using cross-validation\n",
        "print(\"Evaluating best model (Random Forest) on imputed data:\")\n",
        "print(f\"Best parameters: n_estimators={best_rf_model.n_estimators}, max_depth={best_rf_model.max_depth}\")\n",
        "\n",
        "# Cross-validation on imputed data\n",
        "cv_scores_imputed = cross_validate(\n",
        "    best_rf_model, \n",
        "    X_train_missing_preprocessed, \n",
        "    y_train_missing,\n",
        "    cv=5,\n",
        "    scoring=scoring,\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate results\n",
        "cv_results_imputed = {}\n",
        "for metric in scoring.keys():\n",
        "    test_scores = cv_scores_imputed[f'test_{metric}']\n",
        "    train_scores = cv_scores_imputed[f'train_{metric}']\n",
        "    \n",
        "    cv_results_imputed[f'{metric}_test_mean'] = np.mean(test_scores)\n",
        "    cv_results_imputed[f'{metric}_test_std'] = np.std(test_scores)\n",
        "    cv_results_imputed[f'{metric}_train_mean'] = np.mean(train_scores)\n",
        "    cv_results_imputed[f'{metric}_train_std'] = np.std(train_scores)\n",
        "\n",
        "# Test set evaluation on imputed data\n",
        "y_test_missing_pred = best_rf_model.predict(X_test_missing_preprocessed)\n",
        "test_accuracy_imputed = accuracy_score(y_test_missing, y_test_missing_pred)\n",
        "test_f1_imputed = f1_score(y_test_missing, y_test_missing_pred, average='weighted')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE COMPARISON: CLEAN vs IMPUTED DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare with original clean data results\n",
        "original_cv_f1 = final_results['Random Forest']['cv_f1_mean']\n",
        "original_cv_f1_std = final_results['Random Forest']['cv_f1_std']\n",
        "original_test_f1 = final_results['Random Forest']['test_f1']\n",
        "original_test_acc = final_results['Random Forest']['test_accuracy']\n",
        "\n",
        "imputed_cv_f1 = cv_results_imputed['f1_test_mean']\n",
        "imputed_cv_f1_std = cv_results_imputed['f1_test_std']\n",
        "\n",
        "print(f\"{'Metric':<20} {'Clean Data':<20} {'Imputed Data':<20} {'Difference':<15}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'CV F1-Score':<20} {original_cv_f1:.4f} ± {original_cv_f1_std:.4f}    {imputed_cv_f1:.4f} ± {imputed_cv_f1_std:.4f}    {imputed_cv_f1 - original_cv_f1:+.4f}\")\n",
        "print(f\"{'Test F1-Score':<20} {original_test_f1:<20.4f} {test_f1_imputed:<20.4f} {test_f1_imputed - original_test_f1:+.4f}\")\n",
        "print(f\"{'Test Accuracy':<20} {original_test_acc:<20.4f} {test_accuracy_imputed:<20.4f} {test_accuracy_imputed - original_test_acc:+.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nDetailed Classification Report (Imputed Data):\")\n",
        "print(classification_report(y_test_missing, y_test_missing_pred, target_names=['Unhealthy', 'Healthy']))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"IMPACT ANALYSIS:\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Calculate performance degradation\n",
        "f1_degradation = original_test_f1 - test_f1_imputed\n",
        "acc_degradation = original_test_acc - test_accuracy_imputed\n",
        "\n",
        "print(f\"Performance impact of missing data:\")\n",
        "print(f\"  F1-Score degradation: {f1_degradation:.4f} ({f1_degradation/original_test_f1*100:.2f}%)\")\n",
        "print(f\"  Accuracy degradation: {acc_degradation:.4f} ({acc_degradation/original_test_acc*100:.2f}%)\")\n",
        "\n",
        "if abs(f1_degradation) < 0.01:\n",
        "    impact_level = \"Minimal\"\n",
        "elif abs(f1_degradation) < 0.03:\n",
        "    impact_level = \"Small\"\n",
        "elif abs(f1_degradation) < 0.05:\n",
        "    impact_level = \"Moderate\"\n",
        "else:\n",
        "    impact_level = \"Significant\"\n",
        "\n",
        "print(f\"  Overall impact assessment: {impact_level}\")\n",
        "\n",
        "print(f\"\\nImputation strategy effectiveness:\")\n",
        "if f1_degradation < 0.02:\n",
        "    print(f\"  Simple median/mode imputation appears effective\")\n",
        "    print(f\"  Performance maintained within acceptable range\")\n",
        "else:\n",
        "    print(f\"  Notable performance degradation observed\")\n",
        "    print(f\"  Consider more sophisticated imputation methods\")\n",
        "\n",
        "print(f\"\\nMissing data pattern analysis:\")\n",
        "print(f\"  Total missing percentage: {overall_missing_pct:.2f}%\")\n",
        "if overall_missing_pct < 5:\n",
        "    print(f\"  Low missing data rate - simple imputation should suffice\")\n",
        "elif overall_missing_pct < 15:\n",
        "    print(f\"  Moderate missing data rate - results depend on missing pattern\")\n",
        "else:\n",
        "    print(f\"  High missing data rate - advanced imputation methods recommended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 11\n",
        "\n",
        "**Missing Data Analysis:**\n",
        "- 3.31% overall missing data rate (1,324 missing values out of 40,000 cells)\n",
        "- All features have missing values except target variable\n",
        "- `fasting_flag` most affected (11.9% missing), others 1.7-4.6%\n",
        "- 75.9% complete cases, 21.9% with 1 missing value\n",
        "\n",
        "**Imputation Strategy:**\n",
        "- Numerical features: Median imputation\n",
        "- Categorical features: Most frequent imputation\n",
        "- No data leakage: imputers fitted on training data only\n",
        "\n",
        "**Performance Impact:**\n",
        "- Clean data: 86.7% CV F1, 85.0% test F1  \n",
        "- Imputed data: 84.5% CV F1, 84.0% test F1\n",
        "- Performance degradation: 1.0% (1.18% relative decrease)\n",
        "- Impact assessment: Small and acceptable\n",
        "\n",
        "**Key Findings:**\n",
        "- Simple imputation strategy effective for low missing data rate\n",
        "- Performance maintained within acceptable range (< 2% degradation)\n",
        "- Missing data pattern favorable (mostly single missing values per row)\n",
        "elif overall_missing_pct < 15:\n",
        "    print(f\"  Moderate missing data rate - results depend on missing pattern\")\n",
        "else:\n",
        "    print(f\"  High missing data rate - advanced imputation methods recommended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Explainability (Optional)\n",
        "\n",
        "Explainability in machine learning refers to the ability to understand and interpret the decisions made by a model. SHAP (SHapley Additive exPlanations) values provide a unified measure to explain the contribution of each feature to the model's prediction, offering insights into how and why a model makes specific predictions, thus enhancing transparency and trust in complex models.\n",
        "\n",
        "This question is optional. You may choose to address it if you wish to explore the topic or if you are seeking to earn bonus points.\n",
        "\n",
        "(12) **Feature Importance:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 12: Feature Importance with SHAP Analysis (Optional - Bonus Points)\n",
        "\n",
        "# Install SHAP if not already available\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    print(\"SHAP library already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing SHAP library...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\"])\n",
        "    import shap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 12: FEATURE IMPORTANCE ANALYSIS WITH SHAP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Feature names for better interpretation\n",
        "feature_names = list(X_train_preprocessed.columns)\n",
        "print(f\"Features analyzed: {feature_names}\")\n",
        "print(f\"Total features: {len(feature_names)}\")\n",
        "print()\n",
        "\n",
        "# Models to analyze (excluding Random Forest as it's not specifically requested)\n",
        "models_to_analyze = {\n",
        "    'KNN': best_models['KNN'],\n",
        "    'Decision Tree': best_models['Decision Tree'], \n",
        "    'Logistic Regression': best_models['Logistic Regression']\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SHAP ANALYSIS FOR EACH MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store SHAP values for summary\n",
        "shap_results = {}\n",
        "\n",
        "# Create figure for summary plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "for idx, (model_name, model) in enumerate(models_to_analyze.items()):\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Choose appropriate SHAP explainer based on model type\n",
        "        if model_name == 'KNN':\n",
        "            # For KNN, use KernelExplainer with a background dataset\n",
        "            background = shap.sample(X_train_preprocessed, 100)  # Sample for efficiency\n",
        "            explainer = shap.KernelExplainer(model.predict, background)\n",
        "            shap_values = explainer.shap_values(X_test_preprocessed.iloc[:200])  # Limited sample for speed\n",
        "            \n",
        "        elif model_name == 'Decision Tree':\n",
        "            # For tree models, use TreeExplainer\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_test_preprocessed)\n",
        "            # For binary classification, take the positive class\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]\n",
        "                \n",
        "        elif model_name == 'Logistic Regression':\n",
        "            # For linear models, use LinearExplainer  \n",
        "            explainer = shap.LinearExplainer(model, X_train_preprocessed)\n",
        "            shap_values = explainer.shap_values(X_test_preprocessed)\n",
        "        \n",
        "        # Calculate feature importance (mean absolute SHAP values)\n",
        "        if len(shap_values.shape) == 2:\n",
        "            feature_importance = np.abs(shap_values).mean(0)\n",
        "        else:\n",
        "            feature_importance = np.abs(shap_values).mean()\n",
        "            \n",
        "        # Get top 5 features\n",
        "        top_5_indices = np.argsort(feature_importance)[-5:][::-1]\n",
        "        top_5_features = [feature_names[i] for i in top_5_indices]\n",
        "        top_5_importance = feature_importance[top_5_indices]\n",
        "        \n",
        "        print(f\"  Top 5 Most Important Features:\")\n",
        "        for i, (feature, importance) in enumerate(zip(top_5_features, top_5_importance), 1):\n",
        "            print(f\"    {i}. {feature}: {importance:.4f}\")\n",
        "        \n",
        "        # Store results\n",
        "        shap_results[model_name] = {\n",
        "            'shap_values': shap_values,\n",
        "            'feature_importance': feature_importance,\n",
        "            'top_5_features': top_5_features,\n",
        "            'top_5_importance': top_5_importance\n",
        "        }\n",
        "        \n",
        "        # Create summary plot\n",
        "        plt.sca(axes[idx])\n",
        "        \n",
        "        # Create a bar plot of top 5 features\n",
        "        y_pos = np.arange(len(top_5_features))\n",
        "        bars = plt.barh(y_pos, top_5_importance, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "        \n",
        "        plt.yticks(y_pos, top_5_features)\n",
        "        plt.xlabel('Mean |SHAP Value|')\n",
        "        plt.title(f'{model_name}\\nTop 5 Feature Importance')\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, (bar, val) in enumerate(zip(bars, top_5_importance)):\n",
        "            plt.text(val + max(top_5_importance) * 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                    f'{val:.3f}', va='center', ha='left', fontsize=9)\n",
        "        \n",
        "        print(f\"  SHAP analysis completed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Error in SHAP analysis: {str(e)}\")\n",
        "        print(f\"  Skipping {model_name}\")\n",
        "        continue\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Feature Importance Analysis (Top 5 Features per Model)', fontsize=16, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE IMPORTANCE COMPARISON ACROSS MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a comprehensive comparison table\n",
        "if shap_results:\n",
        "    # Get all unique features that appear in top 5 for any model\n",
        "    all_top_features = set()\n",
        "    for results in shap_results.values():\n",
        "        all_top_features.update(results['top_5_features'])\n",
        "    \n",
        "    all_top_features = sorted(list(all_top_features))\n",
        "    \n",
        "    print(f\"{'Feature':<25}\", end=\"\")\n",
        "    for model_name in shap_results.keys():\n",
        "        print(f\"{model_name:<20}\", end=\"\")\n",
        "    print(\"Average Rank\")\n",
        "    print(\"-\" * (25 + 20 * len(shap_results) + 15))\n",
        "    \n",
        "    feature_ranks = {}\n",
        "    for feature in all_top_features:\n",
        "        print(f\"{feature:<25}\", end=\"\")\n",
        "        ranks = []\n",
        "        \n",
        "        for model_name, results in shap_results.items():\n",
        "            if feature in results['top_5_features']:\n",
        "                rank = results['top_5_features'].index(feature) + 1\n",
        "                importance = results['top_5_importance'][results['top_5_features'].index(feature)]\n",
        "                print(f\"#{rank} ({importance:.3f})\"[:19].ljust(20), end=\"\")\n",
        "                ranks.append(rank)\n",
        "            else:\n",
        "                print(\"Not in top 5\".ljust(20), end=\"\")\n",
        "                ranks.append(6)  # Assign rank 6 for features not in top 5\n",
        "        \n",
        "        avg_rank = np.mean(ranks)\n",
        "        feature_ranks[feature] = avg_rank\n",
        "        print(f\"{avg_rank:.1f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"OVERALL FEATURE RANKING (across all models)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Sort features by average rank\n",
        "    sorted_features = sorted(feature_ranks.items(), key=lambda x: x[1])\n",
        "    \n",
        "    print(\"Most consistently important features:\")\n",
        "    for i, (feature, avg_rank) in enumerate(sorted_features[:5], 1):\n",
        "        print(f\"  {i}. {feature} (Average rank: {avg_rank:.1f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INTERPRETATION AND INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if shap_results:\n",
        "    # Analyze patterns across models\n",
        "    print(\"Key findings from SHAP analysis:\")\n",
        "    print()\n",
        "    \n",
        "    # Find most consistent features\n",
        "    consistent_features = [feat for feat, rank in sorted_features[:3]]\n",
        "    print(f\"1. Most Consistent Important Features:\")\n",
        "    for feat in consistent_features:\n",
        "        print(f\"   - {feat}: Appears as important across multiple models\")\n",
        "    \n",
        "    print()\n",
        "    print(f\"2. Model-Specific Insights:\")\n",
        "    \n",
        "    for model_name, results in shap_results.items():\n",
        "        top_feature = results['top_5_features'][0]\n",
        "        top_importance = results['top_5_importance'][0]\n",
        "        \n",
        "        if model_name == 'KNN':\n",
        "            print(f\"   - KNN: Most sensitive to '{top_feature}' (local similarity)\")\n",
        "        elif model_name == 'Decision Tree':\n",
        "            print(f\"   - Decision Tree: Primary split likely on '{top_feature}' (rule-based)\")\n",
        "        elif model_name == 'Logistic Regression':\n",
        "            print(f\"   - Logistic Regression: Strongest linear coefficient for '{top_feature}'\")\n",
        "    \n",
        "    print()\n",
        "    print(f\"3. Feature Type Analysis:\")\n",
        "    \n",
        "    # Categorize features by type\n",
        "    categorical_encoded = [f for f in feature_names if f.startswith('habitat_zone')]\n",
        "    numerical_scaled = [f for f in feature_names if not f.startswith('habitat_zone')]\n",
        "    \n",
        "    cat_in_top = sum(1 for feat in consistent_features if feat in categorical_encoded)\n",
        "    num_in_top = sum(1 for feat in consistent_features if feat in numerical_scaled)\n",
        "    \n",
        "    print(f\"   - Categorical features (encoded): {cat_in_top}/3 in most consistent\")\n",
        "    print(f\"   - Numerical features (scaled): {num_in_top}/3 in most consistent\")\n",
        "    \n",
        "    if num_in_top > cat_in_top:\n",
        "        print(f\"   - Numerical features appear more influential overall\")\n",
        "    elif cat_in_top > num_in_top:\n",
        "        print(f\"   - Categorical features appear more influential overall\")\n",
        "    else:\n",
        "        print(f\"   - Balanced importance between feature types\")\n",
        "\n",
        "else:\n",
        "    print(\"SHAP analysis could not be completed for the selected models.\")\n",
        "    print(\"This may be due to computational constraints or library compatibility issues.\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"SHAP ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results and Analysis - Task 12\n",
        "\n",
        "**SHAP Feature Importance Analysis (Bonus):**\n",
        "- Successfully analyzed KNN and Logistic Regression models (Decision Tree encountered technical limitations)\n",
        "- Used KernelExplainer for KNN and LinearExplainer for Logistic Regression\n",
        "- Generated comparative visualizations showing top 5 features per model\n",
        "\n",
        "**Top Feature Rankings:**\n",
        "1. **habitat_zone_c3**: Most consistently important (rank 1.0 across both models)\n",
        "2. **activity_score**: Second most important (average rank 3.5)\n",
        "3. **enzyme_activity_index**: Strong predictor (average rank 4.0)\n",
        "\n",
        "**Model-Specific Insights:**\n",
        "- **KNN**: habitat_zone_c3 (0.145), stress_variability (0.132), enzyme_activity_index (0.116)\n",
        "- **Logistic Regression**: habitat_zone_c3 (0.683), activity_score (0.291), habitat_zone_c5 (0.229)\n",
        "\n",
        "**Key Findings:**\n",
        "- Categorical habitat zones dominate feature importance (habitat_zone_c3 universally top-ranked)\n",
        "- Numerical features (2/3) appear more consistently influential than categorical features (1/3)\n",
        "- Model algorithms show different sensitivity patterns: KNN emphasizes stress_variability while Logistic Regression prioritizes activity_score\n",
        "- SHAP analysis provides clear interpretability for alien pet health classification decisions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}